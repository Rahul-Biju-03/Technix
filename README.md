# Technix

## Problem Statement
Our Problem statement is “Running GenAI on Intel AI Laptops and Simple LLM Inference on CPU and fine-tuning of LLM Models using Intel® OpenVINO™.”
The challenge lies in efficiently running Generative AI applications and performing LLM inference on Intel AI Laptops and CPUs, while maintaining high performance without specialized hardware. Additionally, fine-tuning LLM models using Intel® OpenVINO™ for real-time applications requires addressing computational efficiency and resource constraints.

## Objective
This project leverages Intel® OpenVINO™ to optimize and execute GenAI and LLM inference on Intel AI Laptops' CPUs, minimizing the reliance on GPUs and enabling efficient, high-performance AI deployment in consumer-grade environments. By fine-tuning LLM models with OpenVINO™, we aim to enhance the performance and accessibility of AI applications. Specifically, we have developed a text generation chatbot using **TinyLlama/TinyLlama-1.1B-Chat-v1.0** to showcase these capabilities.

## Team Members and Contribution
- **Rahul Biju (Team Leader):** CPU Inference
- **Nandakrishnan A:** Model Optimization and Quantization
- **Nandana S Nair:** Project Report
- **Krishna Sagar P:** Project Report
- **Rahul Zachariah:** User Interface Implementation

## Running locally

**1. Clone the repository.**
```bash
git clone https://github.com/Rahul-Biju-03/Technix.git
```

**2. Move into the project directory.**
```bash
cd Technix
```

**3. Install all the required libraries, by installing the requirements.txt file.**
```bash
pip install -r requirements.txt
```

**4. **
```bash
python optimised.py
```

**5. **
```bash
python cpu_inference.py
```

**6. **
```bash
python interface.py
```



## Demo

## Key Components

## References



